{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webpage classification - Fetching data from HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/karthik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/karthik/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "% matplotlib inline\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fetch_data(object):\n",
    "    \"\"\"\n",
    "    This class helps to fetch the data from\n",
    "    the HTML files to a structured one.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, directory):\n",
    "        \"\"\"\n",
    "        The constuctor of the class.\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "        1. directory: The directory of the files.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_frame = pd.DataFrame(columns = [\"URL\", \"Text\", \"University\", \"Label\"])\n",
    "        # dataframe to load data into\n",
    "        self.dir = directory\n",
    "        # input files directory\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        # lemmatizer object\n",
    "        \n",
    "    def __text_process(self, text):\n",
    "        \"\"\"\n",
    "        This function helps to perform text processing\n",
    "        by removing stop words, and lemmatizing the words.\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "        1. self: The object.\n",
    "        2. text: The input text to be processed.\n",
    "        \n",
    "        Return:\n",
    "        1. text: Processed text.\n",
    "        \"\"\"\n",
    "        \n",
    "        text = text.replace(\"cs\", \"computer science\")\n",
    "        # replace cs with computer science\n",
    "        text_lst = text.split(\" \")\n",
    "        # get list of words\n",
    "        text_lst = [self.lemmatizer.lemmatize(word) for word in text_lst if word not in STOPWORDS and len(word) > 1]\n",
    "        # process text\n",
    "        return \" \".join(text_lst)\n",
    "        \n",
    "    def __get_text(self, filename):\n",
    "        \"\"\"\n",
    "        This function helps to get the text\n",
    "        data, and the anchor text for a given\n",
    "        HTML file name.\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "        1. self: The object.\n",
    "        2. filename: The filename from which the data\n",
    "        is to be extracted.\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(filename) as obj:\n",
    "            # open file to read data\n",
    "            try:\n",
    "                data = obj.read()\n",
    "                error = 0\n",
    "            except:\n",
    "                data = \"read error\"\n",
    "                error = 1\n",
    "            # read data from file\n",
    "        data = re.sub('^[^<]+', \"\", data)\n",
    "        #print(data)\n",
    "        #print(\"________________________________________________________\")\n",
    "        # remove top headers from the file\n",
    "        data_bs = BeautifulSoup(data)\n",
    "        #print(data_bs)\n",
    "        #print(\"________________________________________________________\")\n",
    "        # beautiful soup processed data\n",
    "        text = data_bs.get_text()\n",
    "        #print(text)\n",
    "        #print(\"________________________________________________________\")\n",
    "        # get the text data alone Strip special characters and punctuation from a unicode string\n",
    "        \n",
    "        text = text.translate(dict.fromkeys(ord(c) for c in string.punctuation))\n",
    "        # remove all punctuations from the data\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        # replace new line with space\n",
    "        text = re.sub(\"\\d\", \"\", text)\n",
    "        # remove all digits from the text\n",
    "        text = re.sub(\"[\\s]{2,}\", \" \", text).lower()\n",
    "        # replace multiple space with single space\n",
    "        text = self.__text_process(text)\n",
    "        # func call to perform further text processing\n",
    "        return text,error\n",
    "        \n",
    "            \n",
    "    def __get_filename(self):\n",
    "        \"\"\"\n",
    "        This function acts as a generator\n",
    "        to yield filename with complete path\n",
    "        which assists in further process.\n",
    "        \"\"\"\n",
    "        \n",
    "        index = 0\n",
    "        # set index for the dataframe\n",
    "        for path, _, file_lst in os.walk(self.dir):\n",
    "            # iterate over sub-directories to the files\n",
    "            for f in file_lst:\n",
    "                # iterate over each file in the sub-directory\n",
    "                self.data_frame.loc[index, \"URL\"] = f.strip(\"^\")\n",
    "                # add URL to the dataframe\n",
    "                path_lst = path.split(\"/\")\n",
    "                #print(path_lst)\n",
    "                # get path traversed in list\n",
    "                self.data_frame.loc[index, \"University\"] = path_lst[-1]\n",
    "                # add University to the dataframe\n",
    "                self.data_frame.loc[index, \"Label\"] = path_lst[-2]\n",
    "                # add the category to the dataframe\n",
    "                yield index, os.path.join(path, f)\n",
    "                # yield the index, and the file path\n",
    "                index += 1\n",
    "                # update index\n",
    "        \n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        This function traverses the sub-directories\n",
    "        to each filename, and fetches the required data\n",
    "        after pre-processing.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_generator = self.__get_filename()\n",
    "        #print(file_generator)\n",
    "        # generator to get filename\n",
    "        count = 0\n",
    "        for index, file_path in file_generator:\n",
    "            #print(file_path)\n",
    "            # iterate over index, and filepath\n",
    "            text,error = self.__get_text(file_path)\n",
    "            if error:\n",
    "                count += 1\n",
    "            # get text, and anchor contents of the file\n",
    "            self.data_frame.loc[index, \"Text\"] = text.encode(\"utf8\")\n",
    "            # add text data to the dataframe\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crawl = fetch_data(directory = \"webkb\")\n",
    "# initialize object to extract data\n",
    "\n",
    "# NOTE: The argument directory is subjective to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "crawl.get_data()\n",
    "# fetch the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crawl.data_frame.to_csv(path_or_buf = \"data.csv\", index = False)\n",
    "# write data to CSV file named \"data.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
